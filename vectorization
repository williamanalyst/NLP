#
##########################################################################################
# Basic Feature Extractions:
##########################################################################################
# Tokenization: spliting a string into its constituent tokens.
    #
# Lemmatization: convert words into their base form.
    #
#
#



##########################################################################################
# Vectorization:
##########################################################################################
# Bag of Words Model: (when N == 1, N-Grams model is equilvalent to BOW model)
    #
    from sklearn.feature_extraction.text import CountVectorizer
    vectorizer = CountVectorizer(strip_accents = 'ascii', stop_words = 'english', lowercase = True)
    #
    matrix = vectorizer.fit_transform(df1['Description'])
    print(matrix.toarray().shape)
    #
    df_matrix = pd.DataFrame(matrix)
    display(df_matrix)
    display(matrix.toarray()) # or convert to Pandas dataframe if the numpy array could not be displayed
    #
    
    #
# N-Grams Model:
    # 



# Get Similarity (cosine similarity) Score:






##########################################################################################
# Install needed packages and model
pip install spacy
python -m spacy download en_core_web_lg









##########################################################################################
